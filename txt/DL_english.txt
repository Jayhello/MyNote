第三章 概率与信息论  Probability and Information  Theory
Probability theory is a mathematical framework for representing uncertain statements. It provides a means of quantifying uncertainty and axioms for deriving new uncertain statements. In artificial intelligence applications, we use probability theory in two major ways. First, the laws of probability tell us how AI systems should reason, so we design our algorithms to compute or approximate various expressions derived using probability theory. Second, we can use probability and statistics to theoretically analyze the behavior of proposed AI systems.
axiom           n. 公理；格言；自明之理
It’s a very cliched and abused axiom.
这简直就是陈词滥调和滥用公理
The concern about this axiom stemmed from the fact.
对于这个公理的考虑是基于这样的事实
3.2 Random Variables 随机变量
On its own, a random variable is just a description of the states that are possible; it must be coupled with a probability distribution that specifies how likely each of these states are.
一个随机变量只是对可能的状态的描述；它必须伴随着一个概率分布来指定每个状 态的可能性。
Random variables may be discrete or continuous. A discrete random variable is one that has a finite or countably infinite number of states. Note that these states are not necessarily the integers; they can also just be named states that are not considered to have any numerical value. A continuous random variable is associated with a real value.
随机变量可以是离散的或者连续的。离散随机变量拥有有限或者可数无限多的状态。注意这些状态不一定非要是整数；它们也可能只是一些被命名的状态而没有 数值。连续随机变量伴随着实数值。
3.3 Probability Distributions       
A probability distribution is a description of how likely a random variable or set of random variables is to take on each of its possible states. The way we describe probability distributions depends on whether the variables are discrete or continuous
概率分布（probability distribution）用来描述随机变量或一簇随机变量在每一 个可能取到的状态的可能性大小。我们描述概率分布的方式取决于随机变量是离散 的还是连续的Discrete Variables and Probability Mass Functions 离散型变量和概率质量函数
A probability distribution over discrete variables may be described using a proba- bility mass function (PMF). We typically denote probability mass functions with a capital P . Often we associate each random variable with a different probability mass function and the reader must infer which probability mass function to use based on the identity of the random variable, rather than the name of the function; P P ( ) x is usually not the same as ( )
离散型变量的概率分布可以用概率质量函数（probability mass function, PMF） 1 来描述。我们通常用大写字母 P 来表示概率质量函数。通常每一个随机变量都会有 一个不同的概率质量函数，并且读者必须根据随机变量来推断所使用的 PMF，而不 是根据函数的名称来推断；例如，P(x) 通常和 P(y) 不一样
Sometimes to disambiguate which PMF to use, we write the name of the random variable explicitly: P ( x = x ). Sometimes we define a variable first, then use ～ notation to specify which distribution it follows later: x~P(x) .
有时为了使得PMF的使用不相互混淆，我们会明确写出随 机变量的名称：P(x = x)。有时我们会先定义一个随机变量，然后用 ～ 符号来说明 它遵循的分布：x ～ P(x)
Probability mass functions can act on many variables at the same time. Such a probability distribution over many variables is known as a joint probability distribution. P ( x = x,y = y ) denotes the probability that x = x and y = y simultaneously. We may also write for brevity.
概率质量函数可以同时作用于多个随机变量。这种多个变量的概率分布被称 为联合概率分布（joint probability distribution） 。P(x = x,y = y) 表示 x = x 和 y = y 同时发生的概率。我们也可以简写为 P(x,y)。
brevity         n. 短暂；简短；简洁
That, and the relative brevity and insecurity of modern jobs, means your CV has to work harder than ever.
再加上如今工作相对短暂和不安定，意味着你必须在简历上花费比以往更多的心思
To be a probability mass function on a random variable x , a function P must satisfy the following properties:
如果一个函数 P 是随机变量 x 的 PMF，必须满足下面这几个条件：
The domain of must be the set of all possible states of x.
P 的定义域必须是 x 所有可能状态的集合。
? ∈ x x, 0 ≤ P ( x ) ≤ 1 . An impossible event has probability and no state can 0 be less probable than that. Likewise, an event that is guaranteed to happen has probability , and no state can have a greater chance of occurring.
?x ∈ x,0 ≤ P(x) ≤ 1. 不可能发生的事件概率为 0，并且不存在比这概率更低 的状态。类似的，能够确保一定发生的事件概率为 1，而且不存在比这概率更 高的状态。
P x∈x P ( x ) = 1. We refer to this property as being normalized. Without this property, we could obtain probabilities greater than one by computing the probability of one of many events occurring.
∑ x∈x P(x) = 1. 我们把这条性质称之为归一化的（normalized） 。如果没有这 条性质，当我们计算很多事件其中之一发生的概率时可能会得到大于 1 的概 率。
3.3.2 Continuous Variables and Probability Density Functions    连续型变量和概率密度函数
When working with continuous random variables, we describe probability dis- tributions using a probability density function (PDF) rather than a probability mass function.
当我们研究的对象是连续型随机变量时，我们用概率密度函数（probability density function, PDF）而不是概率质量函数来描述它的概率分布
A probability density function p ( x ) does not give the probability of a specific state directly, instead the probability of landing inside an infinitesimal region with volume is given by . δx p x δx
概率密度函数 p(x) 并没有直接对特定的状态给出概率，相对的，它给出了落在 面积为 δx 的无限小的区域内的概率为 p(x)δx。
infinitesimal         adj. 极微的；无限小的n. 极微量  
The body's annual intake of metallic iron is infinitesimal.
人体对金属铁的年摄入量是微不足道的
infinitesimal calculus 无穷小计算infinitesimal analysis 无穷小分析,微元解析infinitesimal change 无限小变化infinitesimal area 无穷小邻域
infinite          adj. 无穷的；无限的n. 无
The number of positive numbers is infinite. 正数的数目是无穷的
an infinite number of 无数的infinite decimal 无穷小数
infinity      n. 无限；无穷；[数]无穷大   infinitive        n. 不定式
Theoretically, a line can extend into infinity.
从理论上来说直线可以无限地延伸
finite     adj. 有限的
The world's resources are finite. 世界的资源是有限的

integrate     v. 整合；结合；取消隔离；求积分adj. 完整的；组合的
Only 20% of teachers report feeling very well prepared to integrate education technology into classroom instruction.
仅有20%25的教师认为他们感觉能够将技术与课堂教学进行整合
A relatively integrate scheme is put forward after many years' research, namely Public Key Infrastructure (PKI), which aims at improving Internet security.
经过多年研究,提出一套完整的安全解决方案,即PKI技术,用于提高网络的安全性能
integrate all activities 融合所有的活动integrate different plans 融合不同的计划integrate completely 完全一体化
interval          n. 间隔；休息时间；(数学)区间；(音乐)音程


mass      n. 大量；块；众多adj. 大规模的；群众的v. 集中 Mass.n. 弥撒曲；弥撒
After harvest we will have a mass of rice.
秋收后，我们将获得大量稻谷
The only correct way is to follow the mass line.
唯一正确的办法是走群众路线
arouse the masses 唤起民众collect into a mass 收集成一堆broad masses 广大群众among the masses 在民众中间


3.4 Marginal Probability       边缘概率
Sometimes we know the probability distribution over a set of variables and we want to know the probability distribution over just a subset of them. The probability distribution over the subset is known as the marginal probability distribution.
有时候，我们知道了一组变量的联合概率分布，但想要了解其中一个子集的概 率分布。这种定义在子集上的概率分布被称为边缘概率分布（marginal probability distribution） 
For example, suppose we have discrete random variables x and y , and we know P , (x y) . We can find x with the :
margin        n. 边缘；余地；幅度；利润；差额；页边空白；定金；【商】原价和卖价之差vt. 加边于；为…付保证金
We came to the margin of the wood.
我们来到树林的边缘。
We allowed a margin of 20 minutes in catching the train.
我们有20分钟的余地赶火车
margin, brim, brink, edge, rim, verge这组词都有“边”“边沿”的意思。1.edge指物体两平面交接处的边缘或边线; brink指悬崖峭壁的边缘; brim指如壶、杯、碗等各种形状容器的内侧边缘,也指帽子的边; rim指任何圆形物体的缘、周或边; verge指一平面或广阔区域的尽头,也常指道路的边缘; margin指书页的空白边缘,也可指物体的边缘。2.用在比喻中, edge指尖锐性、严峻性等; verge表示濒临某种感情或行动的状态; margin的意思是“留有余地”; brink常用于借喻中,指濒临某种危险



The name “marginal probability” comes from the process of computing marginal probabilities on paper. When the values of P ( x y , ) are written in a grid with different values of x in rows and different values of y in columns, it is natural to sum across a row of the grid, then write P ( x ) in the margin of the paper just to the right of the row.
‘‘边缘概率’’ 的名称来源于手算边缘概率的计算过程。当 P(x,y) 的每个值被写 在由每行表示不同的 x 值，每列表示不同的 y 值形成的网格中时，对网格中的每行 求和是很自然的事情，然后将求和的结果 P(x) 写在每行右边的纸的边缘处
For continuous variables, we need to use integration instead of summation:
对于连续型变量，我们需要用积分替代求和：

3.5 Conditional Probability   条件概率
In many cases, we are interested in the probability of some event, given that some other event has happened. This is called a conditional probability. We denote the conditional probability that y = y given x = x as P ( y = y | x = x ). This conditional probability can be computed with the formula
在很多情况下，我们感兴趣的是某个事件，在给定其他事件发生时出现的 概率。这种概率叫做条件概率。我们将给定 x = x，y = y 发生的条件概率记为 P(y = y | x = x)。这个条件概率可以通过下面的公式计算：


3.6 The Chain Rule of Conditional Probabilities  条件概率的链式法则
Any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable:
任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相 乘的形式


3.7 Independence and Conditional Independence  独立性和条件独立性
Two random variables x and y are independent if their probability distribution can be expressed as a product of two factors, one involving only x and one involving only y:
两个随机变量 x 和 y，如果它们的概率分布可以表示成两个因子的乘积形式，并 且一个因子只包含 x 另一个因子只包含 y，我们就称这两个随机变量是相互独立的 （independent） ：

The variance gives a measure of how much the values of a function of a random variable x vary as we sample different values of x from its probability distribution:
方差（variance）衡量的是当我们对 x 依据它的概率分布进行采样时，随机变 量 x 的函数值会呈现多大的差异：


When the variance is low, the values of f ( x ) cluster near their expected value. The square root of the variance is known as the standard deviation.
当方差很小时，f(x) 的值形成的簇比较接近它们的期望值。方差的平方根被称为标 准差（standard deviation） 

deviate     v. 偏离；脱离n. 叛离者；偏差；不正常的人或物adj. 离经叛道的
Don't always deviate to minor issues.
不要老是偏离到次要问题上去
You will not deviate from your present course.
你不能偏离目前航线

The covariance gives some sense of how much two values are linearly related to each other, as well as the scale of these variables:
协方差（covariance）在某种意义上给出了两个变量线性相关性的强度以及这些 变量的尺度：

High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time. If the sign of the covariance is positive, then both variables tend to take on relatively high values simultaneously. If the sign of the covariance is negative, then one variable tends to take on a relatively high value at the times that the other takes on a relatively low value and vice versa.
协方差的绝对值如果很大则意味着变量值变化很大并且它们同时距离各自的均值很 远。如果协方差是正的，那么两个变量都倾向于同时取得相对较大的值。如果协方 差是负的，那么其中一个变量倾向于取得相对较大的值的同时，另一个变量倾向于 取得相对较小的值，反之亦然。
Other measures such as correlation normalize the contribution of each variable in order to measure only how much the variables are related, rather than also being affected by the scale of the separate variables.
其他的衡量指标如相关系数（correlation）将每个变 量的贡献归一化，为了只衡量变量的相关性而不受各个变量尺度大小的影响
3.9 Common Probability Distributions  常用概率分布
Several simple probability distributions are useful in many contexts in machine learning.
许多简单的概率分布在机器学习的众多领域中都是有用的
The distribution is a distribution over a single binary random variable. Bernoulli It is controlled by a single parameter φ ∈ [0 , 1], which gives the probability of the random variable being equal to 1.
Bernoulli 分布（Bernoulli distribution）是单个二值随机变量的分布。它由单 个参数 ? ∈ [0,1] 控制，? 给出了随机变量等于 1 的概率。

The or multinoulli categorical distribution is a distribution over a single discrete variable with k different states, where k is finite. 1 The multinoulli distribution is parametrized by a vector p ∈ [0 , 1] k?1 , where p i gives the probability of the i -th state. The final, k -th state’s probability is given by 1 ? 1 > p. Note that we must constrain 1 > p ≤ 1.
Multinoulli 分布（multinoulli distribution）或者范畴分布（categorical dis- tribution）是指在具有 k 个不同状态的单个离散型随机变量上的分布，其中 k 是一 个有限值。 2 Multinoulli 分布由向量 p ∈ [0,1] k?1 参数化，其中每一个分量 p i 表示 第 i 个状态的概率。最后的第 k 个状态的概率可以通过 1 ? 1 ? p 给出。注意我们必 须限制 1 ? p ≤ 1
The most commonly used distribution over real numbers is the , normal distribution also known as the :
实数上最常用的分布就是正态分布（normal distribution） ，也称为高斯分布 （Gaussian distribution） ：











First, many distributions we wish to model are truly close to being normal distributions. The central limit theorem shows that the sum of many independent random variables is approximately normally distributed. This means that in practice,?many complicated systems can be modeled successfully as normally distributed noise, even if the system can be decomposed into parts with more structured behavior.
第一，我们想要建模的很多分布的真实情况是比较接近正态分布的。中心极限 定理（central limit theorem）说明很多独立随机变量的和近似服从正态分布。这意 味着在实际中，很多复杂系统都可以被成功地建模成正态分布的噪声，即使系统可 以被分解成一些更结构化的部分。

3.9.4 Exponential and Laplace Distributions  指数分布和 Laplace 分布
In the context of deep learning, we often want to have a probability distribution with a sharp point at x = 0. To accomplish this, we can use the exponential distribution:
在深度学习中，我们经常会需要一个在 x = 0 点处取得边界点 (sharp point) 的 分布。为了实现这一目的，我们可以使用指数分布（exponential distribution） ：

The exponential distribution uses the indicator function 1 x≥0 to assign probability zero to all negative values of .
指数分布使用指示函数(indicator function)1 x≥0 来使得当 x 取负值时的概率为零。

A closely related probability distribution that allows us to place a sharp peak of probability mass at an arbitrary point is the μ Laplace distribution
一个联系紧密的概率分布是Laplace 分布（Laplace distribution） ，它允许我们 在任意一点 μ 处设置概率质量的峰值

3.9.5 The Dirac Distribution and Empirical Distribution  Dirac 分布和经验分布
In some cases, we wish to specify that all of the mass in a probability distribution clusters around a single point. This can be accomplished by defining a PDF using the Dirac delta function, :
在一些情况下，我们希望概率分布中的所有质量都集中在一个点上。这可以通 过Dirac delta 函数（Dirac delta function）δ(x) 定义概率密度函数来实现：

The Dirac delta function is defined such that it is zero-valued everywhere except 0, yet integrates to 1. The Dirac delta function is not an ordinary function that associates each value x with a real-valued output, instead it is a different kind of mathematical object called a generalized function that is defined in terms of its properties when integrated. We can think of the Dirac delta function as being the limit point of a series of functions that put less and less mass on all points other than .
Dirac delta 函数被定义成在除了 0 以外的所有点的值都为 0，但是积分为 1。Dirac delta 函数不像普通函数一样对 x 的每一个值都有一个实数值的输出，它是一种不同 类型的数学对象，被称为广义函数（generalized function） ，广义函数是依据积分性 质定义的数学对象。我们可以把 Dirac delta 函数想成一系列函数的极限点，这一系 列函数把除 0 以外的所有点的概率密度越变越小




3.9.6 Mixtures of Distributions分布的混合
It is also common to define probability distributions by combining other simpler probability distributions. One common?way of?combining?distributions is?to construct a mixture distribution. A mixture distribution is made up of several component distributions. On each trial, the choice of which component distribution generates the sample is determined by sampling a component identity from a multinoulli distribution:
通过组合一些简单的概率分布来定义新的概率分布也是很常见的。 一种通用的组 合方法是构造混合分布（mixture distribution） 。混合分布由一些组件 (component) 分布构成。每次实验，样本是由哪个组件分布产生的取决于从一个 Multinoulli 分 布中采样的结果：
3.11 Bayes’ Rule    贝叶斯规则
We often find ourselves in a situation where we know P ( y x | ) and need to know P ( x y | ). Fortunately, if we also know P ( x ), we can compute the desired quantity using :
我们经常会需要在已知 P(y | x) 时计算 P(x | y)。幸运的是，如果还知道 P(x)， 我们可以用贝叶斯规则（Bayes’ rule）来实现这一目的：


Chapter 4 Numerical Computation  数值计算
4.1 Overflow and Underflow    上溢和下溢
The fundamental difficulty in performing continuous math on a digital computer is that we need to represent infinitely many real numbers with a finite number of bit patterns. This means that for almost all real numbers,?we incur some approximation error when we represent the number in the computer. In many cases, this is just rounding error. Rounding error is problematic, especially when it compounds across many operations, and can cause algorithms that work in theory to fail in practice if they are not designed to minimize the accumulation of rounding error.
连续数学在数字计算机上的根本困难是，我们需要通过有限数量的位模式来表 示无限多的实数。这意味着我们在计算机中表示实数时，几乎总会引入一些近似误 差。在许多情况下，这仅仅是舍入误差。舍入误差会导致一些问题，特别是当许多 操作复合时，即使是理论上可行的算法，如果在设计时没有考虑最小化舍入误差的 累积，在实践时也可能会导致算法失效
One form of rounding error that is particularly devastating is . Under- underflow flow occurs when numbers near zero are rounded to zero. Many functions behave qualitatively differently when their argument is zero rather than a small positive number.
一种极具毁灭性的舍入误差是下溢（underflow） 。当接近零的数被四舍五入为 零时发生下溢。许多函数在其参数为零而不是一个很小的正数时才会表现出质的不 同。
Another highly damaging form of numerical error is . Overflow occurs overflow when numbers with large magnitude are approximated as ∞ or ?∞ . Further arithmetic will usually change these infinite values into not-a-number values.
另一个极具破坏力的数值错误形式是上溢（overflow） 。当大量级的数被近似为 ∞ 或 ?∞ 时发生上溢。进一步的运算通常会导致这些无限值变为非数字。



4.2 Poor Conditioning  病态条件
Conditioning refers to how rapidly a function changes with respect to small changes in its inputs. Functions that change rapidly when their inputs are perturbed slightly can be problematic for scientific computation because rounding errors in the inputs can result in large changes in the output.
条件数表征函数相对于输入的微小变化而变化的快慢程度。输入被轻微扰动而 迅速改变的函数对于科学计算来说可能是有问题的，因为输入中的舍入误差可能导 致输出的巨大变化。
Consider the function f ( x ) = A ?1 x . When A ∈ R n n × has an eigenvalue decomposition, its condition number is
考虑函数 f(x) = A ?1 x。当 A ∈ R n×n 具有特征值分解时，其条件数为

This is the ratio of the magnitude of the largest and smallest eigenvalue. When this number is large, matrix inversion is particularly sensitive to error in the input.
这是最大和最小特征值的模之比 1 。当该数很大时，矩阵求逆对输入的误差特别敏感
This sensitivity is an intrinsic property of the matrix itself, not the result of rounding error during matrix inversion. Poorly conditioned matrices amplify pre-existing errors when we multiply by the true matrix inverse. In practice, the error will be compounded further by numerical errors in the inversion process itself.
这种敏感性是矩阵本身的固有特性，而不是矩阵求逆期间舍入误差的结果。即 使我们乘以完全正确的矩阵逆，病态条件的矩阵也会放大预先存在的误差。在实践 中，该错误将与求逆过程本身的数值误差进一步复合


4.3 Gradient-Based Optimization   基于梯度的优化方法
When f 0 ( x ) = 0, the derivative provides no information about which direction to move. Points where f 0 ( x ) = 0 are known as critical points stationary points or . A local minimum is a point where f ( x ) is lower than at all neighboring points, so it is no longer possible to decrease f ( x ) by making infinitesimal steps. A local maximum is a point where f ( x ) is higher than at all neighboring points, so it is not possible to increase f ( x ) by making infinitesimal steps. Some critical points are neither maxima nor minima. These are known as saddle points. See Fig. 4.2 for examples of each type of critical point.
当 f ′ (x) = 0，导数无法提供往哪个方向移动的信息。f ′ (x) = 0 的点称为临界 点（critical point）或驻点（stationary point） 。一个局部极小点（local minimum） 意味着这个点的 f(x) 小于所有邻近点，因此不可能通过移动无穷小的步长来减小 f(x)。一个局部极大点（local maximum）意味着这个点的 f(x) 大于所有邻近点，因 此不可能通过移动无穷小的步长来增大 f(x)。有些临界点既不是最小点也不是最大点。这些点被称为鞍点（saddle point） 。见图4.2给出的各种临界点的例子。


saddle      n. 鞍；车座；山脊；当权vt. 装以马鞍；使负担vi. 装马鞍
The jockey vaulted into the saddle . 骑士一跃而跨坐鞍上
I had to saddle for him the first few times.
头几次我得替他套马鞍
Don't saddle me with taking care of the children.
别叫我干带孩子这种苦差事
saddle用作名词的意思是“鞍子”或类似鞍子的鞍状物,转化为动词意为“套上马鞍”,引申可表示“使(某人)负担”责任、债务等,既可指别人强加于自己的负担,也可指由于自己的过错而造成的负担。
get into saddle 上马jump out of the saddle 跳下马来western saddle (美国)西部牛仔鞍

4.4 Constrained Optimization  约束优化
Sometimes we wish not only to maximize or minimize a function f ( x ) over all possible values of x . Instead we may wish to find the maximal or minimal value of f ( x ) for values of x in some set S . This is known as constrained optimization. Points x that lie within the set S are called feasible points in constrained optimization terminology.
有时候，在 x 的所有可能值下最大化或最小化一个函数 f(x) 不是我们所希望 的。相反，我们可能希望在 x 的某些集合 S 中找 f(x) 的最大值或最小值。这被称 为约束优化（constrained optimization） 。在约束优化术语中，集合 S 内的点 x 被称为可行（feasible）点。 
The KarushCKuhnCTucker (KKT) approach 1 provides a very general solution to constrained optimization. With the KKT approach, we introduce a new function called the generalized Lagrangian generalized Lagrange function or .
KarushCKuhnCTucker（KKT）方法 2 是针对约束优化非常通用的解决方案。 为介绍KKT方法，我们引入一个称为广义 Lagrangian（generalized Lagrangian） 或广义 Lagrange 函数（generalized Lagrange function）的新函数。

4.5 Example: Linear Least Squares实例：线性最小二乘



Chapter 5 Machine Learning Basics  机器学习基础
5.1 Learning Algorithms  学习算法
A machine learning algorithm is an algorithm that is able to learn from data. But what do we mean by learning? ( ) provides the definition “A computer Mitchell 1997 program is said to learn from experience E with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experience E .” 
机器学习算法是一种能够从数据中学习的算法。然而，我们所谓的 ‘‘学习’’ 是什 么意思呢？Mitchell (1997) 提供了一个简洁的定义：‘‘对于某类任务 T 和性能度量 P，一个计算机程序被认为可以从经验 E 中学习是指，通过经验 E 改进后，它在任务 T 上由性能度量 P 衡量的性能有所提升
5.1.1 The Task, T       任务 T
Transcription: In this type of task, the machine learning system is asked to observe a relatively unstructured representation of some kind of data and transcribe it into discrete, textual form. For example, in optical character recognition, the computer program is shown a photograph containing an image of text and is asked to return this text in the form of a sequence of characters (e.g., in ASCII or Unicode format)
转录：这类任务中，机器学习系统观测一些相对非结构化表示的数据，并转 录信息为离散的文本形式。例如，光学字符识别要求计算机程序根据文本图片 返回文字序列（ASCII 码或者 Unicode 码） 。谷歌街景以这种方式使用深度学 习处理街道编号 (Goodfellow et al., 2014d)。
transcript        n. 抄本；副本；誊本；成绩单；笔录  
She went through the transcript carefully in order to eliminate all errors from it.
她把誊本从头至尾认真地看了一遍,以便消除其中的错误之处
We have a transcript of your school records.
我们有一份你的成绩单
transcript unit 转录单位
transcription         n. 抄本；誊写；用音标表示语音；转写；乐曲的改编；(生物)转移
transcription unit 转录单位electric transcription [电] 电唱盘electrical transcription 电气录音,节目录音data transcription 数据记录


Structured output: Structured output tasks involve any task where the output is a vector (or other data structure containing multiple values) with important relationships between the different elements. This is a broad category, and subsumes the transcription and translation tasks described above, but also many other tasks. One example is parsing―mapping a natural language sentence into a tree that describes its grammatical structure and tagging nodes of the trees as being verbs, nouns, or adverbs, and so on. See ( ) Collobert 2011 for an example of deep learning applied to a parsing task. Another example is pixel-wise segmentation of images, where the computer program assigns every pixel in an image to a specific category. For example, deep learning can be used to annotate the locations of roads in aerial photographs (Mnih and Hinton 2010 , ).
结构化输出：结构化输出任务的输出是向量或者其他包含多个值的数据结构， 并且构成输出的这些不同元素间具有重要关系。这是一个很大的范畴，包括上 述转录任务和翻译任务在内的很多其他任务。例如语法分析――映射自然语言 句子到语法结构树，并标记树的节点为动词、名词、副词等等。参考 Collobert (2011) 将深度学习应用到语法分析的示例。另一个例子是图像的像素级分割， 将每一个像素分配到特定类别。例如，深度学习可用于标注航拍照片中的道路位置 (Mnih and Hinton, 2010)

Anomaly detection: In this type of task, the computer program sifts through a set of events or objects, and flags some of them as being unusual or atypical. An example of an anomaly detection task is credit card fraud detection. By modeling your purchasing habits, a credit card company can detect misuse of your cards.?If a thief steals your credit card or credit card information, the thief’s purchases will often come from a different probability distribution over purchase types than your own. The credit card company can prevent fraud by placing a hold on an account as soon as that card has been used for an uncharacteristic purchase
异常检测：在这类任务中，计算机程序在一组事件或对象中筛选，并标记不正 常或非典型的个体。异常检测任务的一个示例是信用卡欺诈检测。通过对你的 购买习惯建模，信用卡公司可以检测到你的卡是否被滥用。如果窃贼窃取你的 信用卡或信用卡信息，窃贼采购物品的分布通常和你的不同。当该卡发生了不 正常的购买行为时，信用卡公司可以尽快冻结该卡以防欺诈
Synthesis and sampling: In this type of task, the machine learning algorithm is asked to generate new examples that are similar to those in the training data. Synthesis and sampling via machine learning can be useful for media applications where it can be expensive or boring for an artist to generate large volumes of content by hand.?For example, video games can automatically generate textures for large objects or landscapes, rather than requiring an artist to manually label each pixel
 合成和采样：在这类任务中，机器学习程序生成一些和训练数据相似的新样本。 通过机器学习，合成和采样可能在媒体应用中非常有用，可以避免艺术家大量 昂贵或者乏味费时的手动工作。例如，视频游戏可以自动生成大型物体或风景 的纹理，而不是让艺术家手动标记每个像素

 去噪：在这类任务中，机器学习算法的输入是，干净样本 x ∈ R n 经过未知损 坏过程后得到的损坏样本 ? x ∈ R n 。算法根据损坏后的样本 ? x 预测干净的样本 x，或者更一般地预测条件概率分布 p(x | ? x)







Density estimation probability mass function estimation or : In the density estimation problem,?the machine learning algorithm is asked to learn a function p model :

where p model ( x ) can be interpreted as a probability density function (if x is continuous) or a probability mass function (if x is discrete) on the space that the examples were drawn from. To do such a task well (we will specify exactly what that means when we discuss performance measures P ),?the?algorithm needs?to learn?the structure?of the?data it has seen. It must know where examples cluster tightly and where they are unlikely to occur. Most of the tasks described above require that the learning algorithm has at?least implicitly captured?the structure?of the probability distribution. Density estimation allows us to explicitly capture that distribution. In principle, we can then perform computations on that distribution in order to solve the other tasks as well. For example, if we have performed density estimation to obtain a probability distribution p ( x ), we can use that distribution to solve the missing value imputation task. If


密度估计或概率质量函数估计：在密度估计问题中，机器学习算法学习函数 p model : R n → R，其中 p model (x) 可以解释成样本采样空间的概率密度函数（如 果 x 是连续的）或者概率质量函数（如果 x 是离散的） 。要做好这样的任务（当我们讨论性能度量 P 时，我们会明确定义任务是什么） ，算法需要学习观测 到的数据的结构。算法必须知道什么情况下样本聚集出现，什么情况下不太可 能出现。以上描述的大多数任务都要求学习算法至少能隐式地捕获概率分布的 结构。密度估计可以让我们显式地捕获该分布。原则上，我们可以在该分布上 计算以便解决其他任务。例如，如果我们通过密度估计得到了概率分布 p(x)， 我们可以用该分布解决缺失值填补任务。如果 x i 的值是缺失的，但是其他的变 量值 x ?i 已知，那么我们可以得到条件概率分布 p(x i | x ?i )。
In practice, density estimation does not always allow us to solve all of these related tasks, because in many cases the required operations on p ( x ) are computationally intractable.
实际情况中，密 度估计并不能够解决所有这类问题，因为在很多情况下 p(x) 是难以计算的


5.1.2 The Performance Measure, P   性能度量 P
5.1.3 The Experience, E   经验 E
Machine learning algorithms can be broadly categorized as unsupervised su- or pervised by what kind of experience they are allowed to have during the learning process.
根据学习过程中的不同经验，机器学习算法可以大致分类为无监督（unsuper- vised）算法和监督（supervised）算法。
Most of the learning algorithms in this book can be understood as being allowed to experience an entire . A dataset is a collection of many examples, as defined in Sec. . Sometimes we will also call examples 5.1.1 data points
本书中的大部分学习算法可以被理解为在整个数据集（dataset）上获取经验。 数据集是指很多样本组成的集合，如第5.1.1节所定义的。有时我们也将样本称为数 据点（data point） 。



5.2.1 The No Free Lunch Theorem     没有免费午餐定理
Learning theory claims that a machine learning algorithm can generalize well from a finite training set of examples. This seems to contradict some basic principles of logic. Inductive reasoning, or inferring general rules from a limited set of examples, is not logically valid.?To logically infer a rule describing every member of a set, one must have information about every member of that set.
学习理论表明机器学习算法能够在有限个训练集样本中很好地泛化。这似乎违 背一些基本的逻辑原则。归纳推理，或是从一组有限的样本中推断一般的规则，在 逻辑上不是很有效。为了逻辑地推断一个规则去描述集合中的元素，我们必须具有 集合中每个元素的信息
5.4 Estimators, Bias and Variance     估计、偏差和方差
The field of statistics gives us many tools that can be used to achieve the machine learning goal of solving a task not only on the training set but also to generalize. Foundational concepts such as parameter estimation, bias and variance are useful to formally characterize notions of generalization, underfitting and overfitting.
统计领域为我们提供了很多工具来实现机器学习目标，不仅可以解决训练集上 的任务，还可以泛化。基本的概念，例如参数估计、偏差和方差，对于正式地刻画泛 化、欠拟合和过拟合都非常有帮助
5.4.1 Point Estimation    点估计
Point estimation is the attempt to provide the single “best” prediction of some quantity of interest. In general the quantity of interest can be a single parameter or a vector of parameters in some parametric model, such as the weights in our linear regression example in Sec. , but it can also be a whole function.
点估计试图为一些感兴趣的量提供单个 ‘‘最优’’ 预测。一般地，感兴趣的量可以 是单个参数，或是某些参数模型中的一个向量参数，例如第5.1.4节线性回归中的权 重，但是也有可能是整个函数

5.4.2 Bias  偏差
The bias of an estimator is defined as:  估计的偏差被定义为


 
5.5 Maximum Likelihood Estimation   最大似然估计

This product over many probabilities can be inconvenient for a variety of reasons. For example, it is prone to numerical underflow. To obtain a more convenient but equivalent optimization problem, we observe that taking the logarithm of the likelihood does not change its argmax but does conveniently transform a product into a sum:
多个概率的乘积会因很多原因不便于计算。例如，计算中很可能会出现数值 下溢。为了得到一个便于计算的等价优化问题，我们观察到似然对数不会改变其 argmax 但是将乘积转化成了便于计算的求和形式：

Because the argmax does not change when we rescale the cost function, we can divide by m to obtain a version of the criterion that is expressed as an expectation with respect to the empirical distribution ? p data defined by the training data:
因为当我们重新缩放代价函数时 argmax 不会改变，我们可以除以 m 得到和训练数 据经验分布 ? p data 相关的期望作为准则：

5.5.1 Conditional Log-Likelihood and Mean Squared Error  条件对数似然和均方误差
Statistical efficiency is typically studied in the parametric case (like in linear regression) where our goal is to estimate the value of a parameter (and assuming it is possible to identify the true parameter), not the value of a function
统计效率通常用于有参情况（parametric case）的研究中（例如线性回归） 。有 参情况中我们的目标是估计参数值（假设有可能确定真实参数） ，而不是函数值

Before observing the data, we represent our knowledge of θ using the prior probability distribution, p ( θ ) (sometimes referred to as simply “the prior”). Gen- erally, the machine learning practitioner selects a prior distribution that is quite broad (i.e. with high entropy) to reflect a high degree of uncertainty in the value of θ before observing any data. For example, one might assume that a priori θ lies in some finite range or volume, with a uniform distribution. Many priors instead reflect a preference for “simpler” solutions (such as smaller magnitude coefficients, or a function that is closer to being constant).
在观察到数据前，我们将 θ 的已知知识表示成先验概率分布（prior probability distribution） ，p(θ)（有时简单地称为 ‘‘先验’’） 。一般而言，机器学习实践者会选择 一个相当宽泛的（即，高熵的）先验分布，反映在观测到任何数据前参数 θ 的高度 不确定性。例如，我们可能会假设先验 θ 在有限区间中均匀分布。许多先验偏好于‘更简单’’ 的解（如小幅度的系数，或是接近常数的函数） 。
5.6.1 Maximum (MAP) Estimation       最大后验 (MAP) 估计
While the most principled approach is to make predictions using the full Bayesian posterior distribution over the parameter θ , it is still often desirable to have a single point estimate.?One common reason for desiring a point estimate is that most operations involving the Bayesian posterior for most interesting models are intractable, and a point estimate offers a tractable approximation. Rather than simply returning to the maximum likelihood estimate, we can still gain some of the benefit of the Bayesian approach by allowing the prior to influence the choice of the point estimate.?One rational way to do this is to choose the maximum a posteriori (MAP) point estimate. The MAP estimate chooses the point of maximal posterior probability (or maximal probability density in the more common case of continuous )
原则上，我们应该使用参数 θ 的完整贝叶斯后验分布进行预测，但单点估计 常常也是需要的。希望使用点估计的一个常见原因是，对于大多数有意义的模型而 言，大多数涉及到贝叶斯后验的计算是非常棘手的，点估计提供了一个可行的近似 解。我们仍然可以让先验影响点估计的选择来利用贝叶斯方法的优点，而不是简单 地回到最大似然估计。一种能够做到这一点的合理方式是选择最大后验（Maximum A Posteriori, MAP）点估计。MAP估计选择后验概率最大的点（或在 θ 是连续值 的更常见情况下，概率密度最大的点） 





























































































